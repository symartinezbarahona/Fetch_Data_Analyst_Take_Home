{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data Analyst Take Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import janitor\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load big query credentials from the service account file\n",
    "credentials = service_account.Credentials.from_service_account_file(config.config_vars['service_account'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part One: Explore data\n",
    "\n",
    "In this section, we will be cleaning the data and processing it into BigQuery for SQL querying. \n",
    "\n",
    "## Cleaning the data\n",
    "\n",
    "The clean_df function is a data cleaning pipeline that processes a CSV file by standardizing column names, removing empty rows and columns, replacing invalid or missing values (such as blank spaces and 'zero'), and converting specific columns to appropriate formats. It converts date columns to datetime, barcode columns to integers, and transaction-related columns to numeric values. Additionally, it removes duplicate rows to ensure data integrity. \n",
    "\n",
    "After applying these transformations, the cleaned DataFrame is returned, making it ready for further analysis or processing. This function helps ensure that the raw data is structured, consistent, and optimized for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and applies common cleaning operations using pyjanitor.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A cleaned DataFrame.\n",
    "    \"\"\"\n",
    "     \n",
    "    df = (\n",
    "        pd.read_csv(file_path)\n",
    "        .clean_names()        \n",
    "        .remove_empty()       \n",
    "        .dropna(how=\"all\")    \n",
    "    )\n",
    "\n",
    "    # Convert blanks into nulls and 'zero' into 0\n",
    "    df.replace({'': np.nan, ' ': np.nan, 'zero': 0}, inplace=True)\n",
    "\n",
    "    # Convert columns with 'date' in their name to datetime\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower(): \n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    # Convert barcode to integer\n",
    "    for col in df.columns:\n",
    "        if 'barcode' in col.lower():\n",
    "            df[col] = df[col].astype('Int64')\n",
    "            \n",
    "    # Convert transaction columns to numeric\n",
    "    for col in df.columns:\n",
    "        if 'final' in col.lower():\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df.drop_duplicates(inplace=True) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = clean_df('USER_TAKEHOME.csv')\n",
    "prod = clean_df('PRODUCTS_TAKEHOME.csv')\n",
    "trans = clean_df('TRANSACTION_TAKEHOME.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into BigQuery\n",
    "\n",
    "The load_gbq function is designed to upload a Pandas DataFrame into a specified dataset within BigQuery. It uses the pandas_gbq.to_gbq() method to load the data into the cloud, specifying the project ID, dataset, and table name from the configuration file. The if_exists='replace' option ensures that any existing table with the same name is replaced with the new data. After successfully loading the data, a message is printed indicating how many rows and columns were uploaded, providing confirmation of the successful operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gbq(df, output_tbl):\n",
    "    \"\"\"\n",
    "    Load a DataFrame to BigQuery.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to load.\n",
    "    - output_tbl (str): The name of the output table in BigQuery.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the DataFrame to BigQuery\n",
    "    pandas_gbq.to_gbq(\n",
    "        df,\n",
    "        destination_table=f\"{config.config_vars['output_dataset']}.{output_tbl}\",\n",
    "        project_id=config.config_vars['project_id'],\n",
    "        if_exists='replace',\n",
    "        credentials=credentials\n",
    "    )\n",
    "\n",
    "    # Print success message with the correct table name\n",
    "    print(f'Loaded {df.shape[0]} rows and {df.shape[1]} columns to {output_tbl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 rows and 6 columns to tbl_users\n",
      "Loaded 845337 rows and 7 columns to tbl_products\n",
      "Loaded 49829 rows and 8 columns to tbl_transactions\n"
     ]
    }
   ],
   "source": [
    "load_gbq(users,'tbl_users')\n",
    "load_gbq(prod,'tbl_products')\n",
    "load_gbq(trans,'tbl_transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part Two: Provide SQL Queries\n",
    "\n",
    "In this section, we will be answering three questions of interest by querying our recently uploaded tables from BigQuery. Special attention will be given to our open-ended question where a fair amount of analysis was conducted.\n",
    "\n",
    "## Close-ended questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client with credentials\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 brands by sales among users that have had their account for at least six months\n",
      "\n",
      "Brand                Total Sales\n",
      "------------------------------\n",
      "CVS                       72.00\n",
      "TRIDENT                   46.72\n",
      "DOVE                      42.88\n",
      "COORS LIGHT               34.96\n",
      "AXE                       15.98\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"\"\"\n",
    "-- Retrieves the top 5 brands by total sales among users who have had their account for at least six months.\n",
    "SELECT \n",
    "  brand \n",
    "  ,ROUND(SUM(final_sale), 2) AS total_sale \n",
    "FROM \n",
    "  `fetch-takehome.fetch_data.tbl_transactions` trans \n",
    "-- Filters users with accounts older than 6 months.\n",
    "INNER JOIN `fetch-takehome.fetch_data.tbl_users` users\n",
    "  ON users.id = trans.user_id\n",
    "  AND CAST(created_date AS DATE) <= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)  -- Filters users who have had an account for at least 6 months\n",
    "-- Retrieves product brand information\n",
    "INNER JOIN `fetch-takehome.fetch_data.tbl_products` prod\n",
    "  ON prod.barcode = trans.barcode \n",
    "  AND brand IS NOT NULL  -- Excludes products with null brands\n",
    "GROUP BY \n",
    "  brand \n",
    "HAVING \n",
    "  total_sale > 0  -- Only include brands with a total sales greater than 0\n",
    "ORDER BY \n",
    "  total_sale DESC  -- Order the results by total sales in descending order to get the highest sales first\n",
    "LIMIT 5;  -- Limit the output to the top 5 brands\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(query_1)\n",
    "\n",
    "# Print header\n",
    "print('Top 5 brands by sales among users that have had their account for at least six months')\n",
    "print()\n",
    "print(f\"{'Brand':<20} {'Total Sales':>10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Loop through the query results and print each row in a table-like format\n",
    "for row in query_job.result():\n",
    "    print(f\"{row['brand']:<20} {row['total_sale']:>10,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales in the Health & Wellness Category by Generation\n",
      "\n",
      "Generation    Total Sales Percentage of Sales\n",
      "------------------------------------------\n",
      "Millennials         59.13               0.31\n",
      "Gen X               41.50               0.22\n",
      "Boomers             89.03               0.47\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"\"\"\n",
    "-- Calculates total sales and percentage of sales by generation for the 'Health & Wellness' category\n",
    "WITH \n",
    "  gen_sales AS (\n",
    "    SELECT \n",
    "      -- Categorizing users based on birth year into generations\n",
    "      CASE \n",
    "        WHEN EXTRACT(YEAR FROM birth_date) BETWEEN 2013 AND 2025 THEN 'Gen Alpha'  \n",
    "        WHEN EXTRACT(YEAR FROM birth_date) BETWEEN 1997 AND 2012 THEN 'Gen Z'        \n",
    "        WHEN EXTRACT(YEAR FROM birth_date) BETWEEN 1981 AND 1996 THEN 'Millennials'  \n",
    "        WHEN EXTRACT(YEAR FROM birth_date) BETWEEN 1965 AND 1980 THEN 'Gen X'        \n",
    "        WHEN EXTRACT(YEAR FROM birth_date) BETWEEN 1946 AND 1964 THEN 'Boomers'      \n",
    "        ELSE 'Other'                                                               \n",
    "      END AS generation\n",
    "      -- Calculating total sales for each generation\n",
    "      ,ROUND(SUM(trans.final_sale),2) AS total_sales\n",
    "    FROM \n",
    "      `fetch-takehome.fetch_data.tbl_transactions` trans\n",
    "    INNER JOIN `fetch-takehome.fetch_data.tbl_users` users\n",
    "      ON users.id = trans.user_id\n",
    "    INNER JOIN `fetch-takehome.fetch_data.tbl_products` prod\n",
    "      ON prod.barcode = trans.barcode \n",
    "      AND prod.category_1 = 'Health & Wellness'\n",
    "    GROUP BY \n",
    "      generation\n",
    ")\n",
    "-- Calculating percentage of sales by generation\n",
    "SELECT \n",
    "  generation\n",
    "  ,total_sales\n",
    "  -- Percentage of total sales by generation\n",
    "  ,ROUND(total_sales / SUM(total_sales) OVER (),2) AS percentage_of_sales\n",
    "FROM \n",
    "gen_sales\n",
    "-- Ordering by the gen hierarchy\n",
    "ORDER BY \n",
    "  CASE \n",
    "    WHEN generation = 'Gen Alpha' THEN 1\n",
    "    WHEN generation = 'Gen Z' THEN 2\n",
    "    WHEN generation = 'Millennials' THEN 3\n",
    "    WHEN generation = 'Gen X' THEN 4\n",
    "    WHEN generation = 'Boomers' THEN 5\n",
    "    ELSE 6\n",
    "  END;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(query_2)\n",
    "\n",
    "# Print header\n",
    "print('Total Sales in the Health & Wellness Category by Generation')\n",
    "print()\n",
    "print(f\"{'Generation':<12} {'Total Sales':>12} {'Percentage of Sales':>18}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Loop through the query results and print each row in a table-like format\n",
    "for row in query_job.result():\n",
    "    print(f\"{row['generation']:<12} {row['total_sales']:>12,.2f} {row['percentage_of_sales']:>18,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-ended questions\n",
    "\n",
    "To determine Fetch’s power users, we will use the RFM (Recency, Frequency, Monetary) methodology, a marketing analysis tool used to identify a company's best customers by measuring and examining how recently a customer has purchased (Recency), how often they purchase (Frequency), how much the customer spends (Monetary). The table below outlines how each metric is defined in the context of Fetch.\n",
    "\n",
    "Metric | Definition\n",
    "--- | -----------\n",
    "Recency | # of days since last receipt scanned\n",
    "Frequency | # of unique receipts scanned\n",
    "Monetary | Total sum of revenue from user transactions\n",
    "\n",
    "Each metric is divided into three buckets based on ntile() and scored from 1 to 3, where 1 indicates the best performance. These scores are then summed to produce a cumulative score (k), which is used to classify customers into segments.\n",
    "\n",
    "Segmentation | \tCumulative Score (k)\n",
    "--- | -----------\n",
    "High | ≥ 8\n",
    "Medium | 6 - 7\n",
    "Low | < 6\n",
    "\n",
    "Advantages:\n",
    "- Direct Revenue Insight: RFM analysis, particularly with the monetary component, provides direct insight into how much value users contribute to the app in terms of spending, helping prioritize high-revenue users.\n",
    "- High-Value User Identification: The RFM model helps identify high-value users who engage frequently or make significant purchases, allowing Fetch to focus on the most active and valuable customers.\n",
    "- Targeted Marketing: RFM scores enable more effective marketing campaigns, such as targeting high-scoring users with exclusive offers or loyalty programs to retain their engagement.\n",
    "- Comprehensive Engagement Tracking: By combining Recency (how recently users engaged), Frequency (how often they engage), and Monetary (how much they spend), RFM provides a complete view of user behavior, identifying both frequent and high-value users.\n",
    "\n",
    "Potential Bias:\n",
    "- Monetary Bias: Users who purchase fewer, more expensive items may score higher in the monetary category, even if their overall engagement is low, skewing results by prioritizing high spenders over highly engaged users.\n",
    "- Inconsistent Spending Patterns: Some users may make infrequent but large purchases, placing them in the high monetary category despite lower overall engagement, misrepresenting their loyalty.\n",
    "- Excluding Non-Revenue Users: Users who engage heavily but do not directly contribute to revenue (e.g., those who use app features but do not purchase) may be undervalued or overlooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFM Segment Analysis\n",
      "\n",
      "RFM Segment    User Count   Total User Count   Percent of Total\n",
      "------------------------------------------------------------\n",
      "1                    2462              17518               0.14\n",
      "2                    8677              17518               0.50\n",
      "3                    6379              17518               0.36\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"\"\"\n",
    "-- Produces a K score for each user based on recency, frequency, and monetary metrics. It then determines the number of users who are considered powers users. \n",
    "WITH\n",
    "  -- Calculate Recency, Frequency, and Monetary metrics for each user\n",
    "  rfm_data AS (\n",
    "    SELECT\n",
    "      user_id\n",
    "      ,MAX(CAST(scan_date AS DATE)) AS scan_date\n",
    "      ,DATE_DIFF(CAST(MAX(scan_date) AS DATE), CURRENT_DATE(),DAY) AS recency -- Difference in days from last transaction to current date\n",
    "      ,COUNT(DISTINCT receipt_id) AS frequency -- Number of transactions\n",
    "      ,ROUND(SUM(final_sale), 2) AS monetary --Total amount spent by user\n",
    "    FROM \n",
    "      `fetch-takehome.fetch_data.tbl_transactions` trans\n",
    "    GROUP BY \n",
    "      user_id\n",
    "    HAVING \n",
    "      monetary > 0 -- only include users with a total sales greater than 0\n",
    "  ),\n",
    "  -- Apply NTILE function to divide users into 3 buckets for each RFM metric\n",
    "  tiles AS (\n",
    "    SELECT \n",
    "      user_id \n",
    "      ,NTILE(3) OVER (ORDER BY recency DESC) AS ptile_recency\n",
    "      ,NTILE(3) OVER (ORDER BY frequency) AS ptile_frequency\n",
    "      ,NTILE(3) OVER (ORDER BY monetary) AS ptile_monetary\n",
    "    FROM\n",
    "      rfm_data\n",
    "  ), \n",
    "  -- Calculate cumulative RFM score and classify users into segments where 1 is high value and 3 is low value\n",
    "  rfm_buckets AS (\n",
    "      SELECT *\n",
    "      ,ptile_recency + ptile_frequency + ptile_monetary AS cumulative_sum\n",
    "      ,CASE \n",
    "        WHEN (ptile_recency + ptile_frequency + ptile_monetary) >= 8 THEN 1\n",
    "        WHEN (ptile_recency + ptile_frequency + ptile_monetary) < 6 THEN 3\n",
    "        ELSE 2\n",
    "        END AS rfm_segment\n",
    "      FROM\n",
    "        tiles\n",
    "  )\n",
    "-- Aggregate results and calculate percentage of total users per segment\n",
    "SELECT\n",
    "  rfm_segment\n",
    "  ,COUNT(*) AS user_count\n",
    "  ,SUM(COUNT(*)) OVER() AS total_user_count\n",
    "  ,ROUND(COUNT(*)/SUM(COUNT(*)) OVER(),2) AS percent_of_total\n",
    "FROM\n",
    "  rfm_buckets\n",
    "GROUP BY \n",
    "  rfm_segment\n",
    "ORDER BY \n",
    "  rfm_segment\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(query_3)\n",
    "\n",
    "# Print header\n",
    "print('RFM Segment Analysis')\n",
    "print()\n",
    "print(f\"{'RFM Segment':<12} {'User Count':>12} {'Total User Count':>18} {'Percent of Total':>18}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Loop through the query results and print each row in a table-like format\n",
    "for row in query_job.result():\n",
    "    print(f\"{row['rfm_segment']:<12} {row['user_count']:>12} {row['total_user_count']:>18} {row['percent_of_total']:>18.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "takehome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
